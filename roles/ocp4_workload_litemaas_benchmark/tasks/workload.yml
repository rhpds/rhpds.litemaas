---
# ============================================================================
# LiteMaaS Benchmark Workload Tasks
# ============================================================================
# Runs multi-turn conversation benchmarks to validate LiteMaaS performance
# ============================================================================

- name: Get LiteMaaS credentials from virtual keys workload
  ansible.builtin.set_fact:
    _benchmark_litellm_url: "{{ lookup('agnosticd_user_data', 'litellm_api_base_url') }}"
    _benchmark_litellm_key: "{{ lookup('agnosticd_user_data', 'litellm_virtual_key') }}"

- name: Validate benchmark configuration
  ansible.builtin.assert:
    that:
      - _benchmark_litellm_url is defined
      - _benchmark_litellm_url | length > 0
      - _benchmark_litellm_key is defined
      - _benchmark_litellm_key | length > 0
    fail_msg: |
      LiteMaaS URL or API key not configured.
      Ensure rhpds.litellm_virtual_keys workload ran successfully first.
      URL: {{ _benchmark_litellm_url | default('NOT SET') }}
      Key: {{ 'SET' if _benchmark_litellm_key is defined and _benchmark_litellm_key | length > 0 else 'NOT SET' }}

- name: Save benchmark configuration to user.info data
  agnosticd.core.agnosticd_user_info:
    data:
      litemaas_benchmark_namespace: "{{ ocp4_workload_litemaas_benchmark_namespace }}"
      litemaas_benchmark_conversations: "{{ ocp4_workload_litemaas_benchmark_conversations }}"
      litemaas_benchmark_turns: "{{ ocp4_workload_litemaas_benchmark_turns }}"

- name: Display benchmark start message
  agnosticd.core.agnosticd_user_info:
    msg: |

      â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
      LiteMaaS Benchmark Starting
      â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

      Configuration:
        â€¢ Conversations:  {{ ocp4_workload_litemaas_benchmark_conversations }}
        â€¢ Turns:          {{ ocp4_workload_litemaas_benchmark_turns }}
        â€¢ Parallel:       {{ ocp4_workload_litemaas_benchmark_parallel_workers }}
        â€¢ Max Tokens:     {{ ocp4_workload_litemaas_benchmark_max_tokens }}
        â€¢ Request Delay:  {{ ocp4_workload_litemaas_benchmark_min_delay }}-{{ ocp4_workload_litemaas_benchmark_max_delay }}s

      Endpoint: {{ _benchmark_litellm_url }}

      Estimated Duration:
        {{ (ocp4_workload_litemaas_benchmark_conversations * ocp4_workload_litemaas_benchmark_turns * 10 / 60) | int }} minutes

      â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

- name: Create benchmark namespace
  kubernetes.core.k8s:
    state: present
    definition:
      apiVersion: v1
      kind: Namespace
      metadata:
        name: "{{ ocp4_workload_litemaas_benchmark_namespace }}"
        labels:
          guid: "{{ guid }}"
          workload: ocp4_workload_litemaas_benchmark

- name: Create benchmark Job
  kubernetes.core.k8s:
    state: present
    definition:
      apiVersion: batch/v1
      kind: Job
      metadata:
        name: litemaas-benchmark
        namespace: "{{ ocp4_workload_litemaas_benchmark_namespace }}"
        labels:
          app: litemaas-benchmark
          guid: "{{ guid }}"
          workload: ocp4_workload_litemaas_benchmark
      spec:
        backoffLimit: 1
        ttlSecondsAfterFinished: 86400  # Keep for 24 hours
        template:
          metadata:
            labels:
              app: litemaas-benchmark
              guid: "{{ guid }}"
          spec:
            restartPolicy: Never
            containers:
              - name: benchmark
                image: "{{ ocp4_workload_litemaas_benchmark_image }}"
                args:
                  - "{{ _benchmark_litellm_url }}"
                  - "--conversations"
                  - "{{ ocp4_workload_litemaas_benchmark_conversations | string }}"
                  - "--turns"
                  - "{{ ocp4_workload_litemaas_benchmark_turns | string }}"
                  - "--parallel"
                  - "{{ ocp4_workload_litemaas_benchmark_parallel_workers | string }}"
                  - "--max-tokens"
                  - "{{ ocp4_workload_litemaas_benchmark_max_tokens | string }}"
                  - "--min-delay"
                  - "{{ ocp4_workload_litemaas_benchmark_min_delay | string }}"
                  - "--max-delay"
                  - "{{ ocp4_workload_litemaas_benchmark_max_delay | string }}"
                env:
                  - name: OPENAI_API_KEY
                    value: "{{ _benchmark_litellm_key }}"
                resources:
                  requests:
                    cpu: "500m"
                    memory: "512Mi"
                  limits:
                    cpu: "2"
                    memory: "2Gi"

- name: Save benchmark job info
  when: ocp4_workload_litemaas_benchmark_save_results | bool
  agnosticd.core.agnosticd_user_info:
    data:
      litemaas_benchmark_namespace: "{{ ocp4_workload_litemaas_benchmark_namespace }}"
      litemaas_benchmark_job_name: "litemaas-benchmark"
      litemaas_benchmark_conversations: "{{ ocp4_workload_litemaas_benchmark_conversations }}"
      litemaas_benchmark_turns: "{{ ocp4_workload_litemaas_benchmark_turns }}"

- name: Wait for benchmark to complete
  kubernetes.core.k8s_info:
    api_version: batch/v1
    kind: Job
    name: litemaas-benchmark
    namespace: "{{ ocp4_workload_litemaas_benchmark_namespace }}"
  register: benchmark_job
  until:
    - benchmark_job.resources | length > 0
    - benchmark_job.resources[0].status.succeeded is defined or benchmark_job.resources[0].status.failed is defined
  retries: 120
  delay: 30
  ignore_errors: true

- name: Check if benchmark succeeded
  ansible.builtin.set_fact:
    benchmark_succeeded: "{{ (benchmark_job.resources | default([]) | length > 0) and (benchmark_job.resources[0].status.succeeded | default(0) == 1) }}"
    benchmark_failed: "{{ (benchmark_job.resources | default([]) | length > 0) and (benchmark_job.resources[0].status.failed | default(0) == 1) }}"

- name: Get benchmark pod name
  when: benchmark_succeeded or benchmark_failed
  kubernetes.core.k8s_info:
    api_version: v1
    kind: Pod
    namespace: "{{ ocp4_workload_litemaas_benchmark_namespace }}"
    label_selectors:
      - app=litemaas-benchmark
  register: benchmark_pods

- name: Fetch benchmark results
  when:
    - benchmark_pods is defined
    - benchmark_pods.resources is defined
    - benchmark_pods.resources | length > 0
    - benchmark_succeeded or benchmark_failed
  kubernetes.core.k8s_log:
    api_version: v1
    kind: Pod
    name: "{{ benchmark_pods.resources[0].metadata.name }}"
    namespace: "{{ ocp4_workload_litemaas_benchmark_namespace }}"
  register: benchmark_output
  ignore_errors: true

- name: Parse benchmark metrics
  when: benchmark_succeeded and benchmark_output is defined
  block:
    - name: Extract metrics from output
      ansible.builtin.set_fact:
        benchmark_p95: "{{ benchmark_output.log | regex_search('P95:\\s+(\\d+\\.\\d+)', '\\1') | first | default('N/A') }}"
        benchmark_p99: "{{ benchmark_output.log | regex_search('P99:\\s+(\\d+\\.\\d+)', '\\1') | first | default('N/A') }}"
        benchmark_mean: "{{ benchmark_output.log | regex_search('Mean:\\s+(\\d+\\.\\d+)', '\\1') | first | default('N/A') }}"
        benchmark_p50: "{{ benchmark_output.log | regex_search('P50:\\s+(\\d+\\.\\d+)', '\\1') | first | default('N/A') }}"
        benchmark_speedup: "{{ benchmark_output.log | regex_search('Speedup ratio:\\s+(\\d+\\.\\d+)', '\\1') | first | default('N/A') }}"
        benchmark_total_time: "{{ benchmark_output.log | regex_search('Total time:\\s+(\\d+\\.\\d+)s', '\\1') | first | default('N/A') }}"
        benchmark_total_requests: "{{ benchmark_output.log | regex_search('Total requests:\\s+(\\d+)', '\\1') | first | default('N/A') }}"
        benchmark_rps: "{{ benchmark_output.log | regex_search('Requests per second:\\s+(\\d+\\.\\d+)', '\\1') | first | default('N/A') }}"

    - name: Determine benchmark status
      ansible.builtin.set_fact:
        benchmark_status: >-
          {{ 'PASS âœ“' if (benchmark_p95 != 'N/A' and benchmark_p95 | float < ocp4_workload_litemaas_benchmark_ttft_p95_threshold_ms)
             else 'WARN âš ' if (benchmark_p95 != 'N/A' and benchmark_p95 | float < (ocp4_workload_litemaas_benchmark_ttft_p95_threshold_ms * 2))
             else 'FAIL âœ—' }}
        benchmark_cache_status: >-
          {{ 'EXCELLENT âœ“' if (benchmark_speedup != 'N/A' and benchmark_speedup | float > (ocp4_workload_litemaas_benchmark_speedup_threshold * 1.5))
             else 'GOOD âœ“' if (benchmark_speedup != 'N/A' and benchmark_speedup | float > ocp4_workload_litemaas_benchmark_speedup_threshold)
             else 'POOR âœ—' }}

    - name: Save benchmark results to user.info data
      when: ocp4_workload_litemaas_benchmark_save_results | bool
      agnosticd.core.agnosticd_user_info:
        data:
          litemaas_benchmark_p50_ms: "{{ benchmark_p50 }}"
          litemaas_benchmark_p95_ms: "{{ benchmark_p95 }}"
          litemaas_benchmark_p99_ms: "{{ benchmark_p99 }}"
          litemaas_benchmark_mean_ms: "{{ benchmark_mean }}"
          litemaas_benchmark_speedup_ratio: "{{ benchmark_speedup }}"
          litemaas_benchmark_status: "{{ benchmark_status }}"
          litemaas_benchmark_cache_status: "{{ benchmark_cache_status }}"
          litemaas_benchmark_total_time_seconds: "{{ benchmark_total_time }}"
          litemaas_benchmark_total_requests: "{{ benchmark_total_requests }}"
          litemaas_benchmark_requests_per_second: "{{ benchmark_rps }}"

- name: Display benchmark results (Success)
  when: benchmark_succeeded and benchmark_output is defined
  agnosticd.core.agnosticd_user_info:
    msg: |

      â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
      LiteMaaS Benchmark Results
      â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

      Time to First Token (TTFT):
        â€¢ Mean:           {{ benchmark_mean }} ms
        â€¢ P50 (Median):   {{ benchmark_p50 }} ms
        â€¢ P95:            {{ benchmark_p95 }} ms
        â€¢ P99:            {{ benchmark_p99 }} ms

      Cache Performance:
        â€¢ Speedup Ratio:  {{ benchmark_speedup }}x
        â€¢ Status:         {{ benchmark_cache_status }}

      Load Testing Results:
        â€¢ Total Time:     {{ benchmark_total_time }} seconds
        â€¢ Total Requests: {{ benchmark_total_requests }}
        â€¢ Requests/sec:   {{ benchmark_rps }}

      Test Configuration:
        â€¢ Conversations:  {{ ocp4_workload_litemaas_benchmark_conversations }}
        â€¢ Turns:          {{ ocp4_workload_litemaas_benchmark_turns }}
        â€¢ Parallel:       {{ ocp4_workload_litemaas_benchmark_parallel_workers }}
        â€¢ Max Tokens:     {{ ocp4_workload_litemaas_benchmark_max_tokens }}

      Overall Status: {{ benchmark_status }}

      Performance Interpretation:
        TTFT P95 < {{ ocp4_workload_litemaas_benchmark_ttft_p95_threshold_ms }}ms:   Excellent user experience
        TTFT P95 < {{ ocp4_workload_litemaas_benchmark_ttft_p95_threshold_ms * 2 }}ms:  Good user experience
        TTFT P95 > {{ ocp4_workload_litemaas_benchmark_ttft_p95_threshold_ms * 2 }}ms:  May need optimization

        Speedup > {{ ocp4_workload_litemaas_benchmark_speedup_threshold * 1.5 }}x:  Excellent prefix caching
        Speedup > {{ ocp4_workload_litemaas_benchmark_speedup_threshold }}x:    Good prefix caching
        Speedup < {{ ocp4_workload_litemaas_benchmark_speedup_threshold }}x:    Poor cache effectiveness

      Full Results:
        oc logs -n {{ ocp4_workload_litemaas_benchmark_namespace }} -l app=litemaas-benchmark

      â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

- name: Deploy benchmark report web page
  when: benchmark_succeeded and benchmark_output is defined
  block:
    - name: Render HTML report
      ansible.builtin.template:
        src: benchmark-report.html.j2
        dest: /tmp/benchmark-report-{{ guid }}.html
      vars:
        benchmark_output_raw: "{{ benchmark_output.log }}"

    - name: Create ConfigMap with HTML report
      kubernetes.core.k8s:
        state: present
        definition:
          apiVersion: v1
          kind: ConfigMap
          metadata:
            name: benchmark-report
            namespace: "{{ ocp4_workload_litemaas_benchmark_namespace }}"
            labels:
              app: benchmark-report
              guid: "{{ guid }}"
          data:
            index.html: "{{ lookup('file', '/tmp/benchmark-report-' + guid + '.html') }}"

    - name: Create nginx Deployment for report
      kubernetes.core.k8s:
        state: present
        definition:
          apiVersion: apps/v1
          kind: Deployment
          metadata:
            name: benchmark-report
            namespace: "{{ ocp4_workload_litemaas_benchmark_namespace }}"
            labels:
              app: benchmark-report
              guid: "{{ guid }}"
          spec:
            replicas: 1
            selector:
              matchLabels:
                app: benchmark-report
            template:
              metadata:
                labels:
                  app: benchmark-report
              spec:
                containers:
                - name: nginx
                  image: registry.access.redhat.com/ubi9/nginx-124:latest
                  ports:
                  - containerPort: 8080
                    protocol: TCP
                  volumeMounts:
                  - name: report
                    mountPath: /usr/share/nginx/html
                    readOnly: true
                volumes:
                - name: report
                  configMap:
                    name: benchmark-report

    - name: Create Service for report
      kubernetes.core.k8s:
        state: present
        definition:
          apiVersion: v1
          kind: Service
          metadata:
            name: benchmark-report
            namespace: "{{ ocp4_workload_litemaas_benchmark_namespace }}"
            labels:
              app: benchmark-report
              guid: "{{ guid }}"
          spec:
            selector:
              app: benchmark-report
            ports:
            - port: 8080
              targetPort: 8080
              protocol: TCP

    - name: Create Route for report
      kubernetes.core.k8s:
        state: present
        definition:
          apiVersion: route.openshift.io/v1
          kind: Route
          metadata:
            name: benchmark-report
            namespace: "{{ ocp4_workload_litemaas_benchmark_namespace }}"
            labels:
              app: benchmark-report
              guid: "{{ guid }}"
          spec:
            to:
              kind: Service
              name: benchmark-report
            port:
              targetPort: 8080
            tls:
              termination: edge
              insecureEdgeTerminationPolicy: Redirect
      register: benchmark_report_route

    - name: Get Route URL
      kubernetes.core.k8s_info:
        api_version: route.openshift.io/v1
        kind: Route
        name: benchmark-report
        namespace: "{{ ocp4_workload_litemaas_benchmark_namespace }}"
      register: report_route_info

    - name: Save report URL to user.info
      when: report_route_info.resources | length > 0
      agnosticd.core.agnosticd_user_info:
        data:
          litemaas_benchmark_report_url: "https://{{ report_route_info.resources[0].spec.host }}"

    - name: Display report URL
      when: report_route_info.resources | length > 0
      agnosticd.core.agnosticd_user_info:
        msg: |

          â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
          ğŸ“Š View Full Benchmark Report
          â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

          https://{{ report_route_info.resources[0].spec.host }}

          This interactive report includes:
            â€¢ Visual metrics and charts
            â€¢ Performance interpretation
            â€¢ Full benchmark output
            â€¢ Configuration details

          â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

- name: Display benchmark failure
  when: benchmark_failed
  agnosticd.core.agnosticd_user_info:
    msg: |

      â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
      LiteMaaS Benchmark Failed
      â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

      The benchmark job failed to complete.

      Troubleshooting:
        # Check job status
        oc get job litemaas-benchmark -n {{ ocp4_workload_litemaas_benchmark_namespace }}

        # View error logs
        oc logs -n {{ ocp4_workload_litemaas_benchmark_namespace }} -l app=litemaas-benchmark

        # Describe pod for events
        oc describe pod -n {{ ocp4_workload_litemaas_benchmark_namespace }} -l app=litemaas-benchmark

      Common Issues:
        - LiteMaaS endpoint unreachable
        - Invalid API key
        - Model not available
        - Context length exceeded (reduce max_tokens)
        - Network connectivity issues

      â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
