---
# ============================================================================
# LiteMaaS Benchmark Workload Tasks
# ============================================================================
# Runs multi-turn conversation benchmarks to validate LiteMaaS performance
# ============================================================================

- name: Get LiteMaaS credentials from virtual keys workload
  ansible.builtin.set_fact:
    _benchmark_litellm_url: "{{ lookup('agnosticd_user_data', 'litellm_api_base_url') }}"
    _benchmark_litellm_key: "{{ lookup('agnosticd_user_data', 'litellm_virtual_key') }}"

- name: Validate benchmark configuration
  ansible.builtin.assert:
    that:
      - _benchmark_litellm_url is defined
      - _benchmark_litellm_url | length > 0
      - _benchmark_litellm_key is defined
      - _benchmark_litellm_key | length > 0
    fail_msg: |
      LiteMaaS URL or API key not configured.
      Ensure rhpds.litellm_virtual_keys workload ran successfully first.
      URL: {{ _benchmark_litellm_url | default('NOT SET') }}
      Key: {{ 'SET' if _benchmark_litellm_key is defined and _benchmark_litellm_key | length > 0 else 'NOT SET' }}

- name: Calculate parallel workers if not specified (auto-calculate based on attendees)
  ansible.builtin.set_fact:
    _benchmark_parallel_workers: >-
      {{
        ocp4_workload_litemaas_benchmark_parallel_workers
        if (ocp4_workload_litemaas_benchmark_parallel_workers is defined and ocp4_workload_litemaas_benchmark_parallel_workers != none)
        else (
          16 if (ocp4_workload_litemaas_benchmark_conversations | int > 100)
          else 8 if (ocp4_workload_litemaas_benchmark_conversations | int > 60)
          else 4 if (ocp4_workload_litemaas_benchmark_conversations | int > 30)
          else 2
        )
      }}

- name: Calculate effective turns (apply 3x multiplier if MCP enabled)
  ansible.builtin.set_fact:
    _benchmark_effective_turns: >-
      {{
        (ocp4_workload_litemaas_benchmark_turns | int * 3)
        if (ocp4_workload_litemaas_benchmark_mcp_enabled | default(false) | bool)
        else ocp4_workload_litemaas_benchmark_turns | int
      }}

- name: Save benchmark configuration to user.info data
  agnosticd.core.agnosticd_user_info:
    data:
      litemaas_benchmark_namespace: "{{ ocp4_workload_litemaas_benchmark_namespace }}"
      litemaas_benchmark_conversations: "{{ ocp4_workload_litemaas_benchmark_conversations }}"
      litemaas_benchmark_sessions: "{{ ocp4_workload_litemaas_benchmark_sessions }}"
      litemaas_benchmark_turns: "{{ ocp4_workload_litemaas_benchmark_turns }}"
      litemaas_benchmark_effective_turns: "{{ _benchmark_effective_turns }}"
      litemaas_benchmark_mcp_enabled: "{{ ocp4_workload_litemaas_benchmark_mcp_enabled | default(false) }}"
      litemaas_benchmark_parallel_workers: "{{ _benchmark_parallel_workers }}"

- name: Display benchmark start message
  agnosticd.core.agnosticd_user_info:
    msg: |

      â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
      LiteMaaS Benchmark Starting
      â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

      Workshop Configuration:
        â€¢ Attendees/Session:  {{ ocp4_workload_litemaas_benchmark_conversations }}
        â€¢ Number of Sessions: {{ ocp4_workload_litemaas_benchmark_sessions }}
        â€¢ Questions/Person:   {{ ocp4_workload_litemaas_benchmark_turns }}
        {% if ocp4_workload_litemaas_benchmark_mcp_enabled | default(false) | bool %}
        â€¢ Tool Calling (MCP): âœ“ Enabled (3x multiplier)
        â€¢ Effective Queries:  {{ _benchmark_effective_turns }} ({{ ocp4_workload_litemaas_benchmark_turns }} Ã— 3)
        {% endif %}

      Test Configuration:
        â€¢ Parallel Workers:   {{ _benchmark_parallel_workers }}{% if ocp4_workload_litemaas_benchmark_parallel_workers is not defined or ocp4_workload_litemaas_benchmark_parallel_workers == none %} (auto-calculated){% endif %}
        â€¢ Max Tokens:         {{ ocp4_workload_litemaas_benchmark_max_tokens }}
        â€¢ Request Delay:      {{ ocp4_workload_litemaas_benchmark_min_delay }}-{{ ocp4_workload_litemaas_benchmark_max_delay }}s

      Endpoint: {{ _benchmark_litellm_url }}

      Estimated Duration:
        {{ (_benchmark_effective_turns | int * ocp4_workload_litemaas_benchmark_conversations | int * 10 / 60) | int }} minutes

      â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

- name: Create benchmark namespace
  kubernetes.core.k8s:
    state: present
    definition:
      apiVersion: v1
      kind: Namespace
      metadata:
        name: "{{ ocp4_workload_litemaas_benchmark_namespace }}"
        labels:
          guid: "{{ guid }}"
          workload: ocp4_workload_litemaas_benchmark

- name: Create benchmark Job
  kubernetes.core.k8s:
    state: present
    definition:
      apiVersion: batch/v1
      kind: Job
      metadata:
        name: litemaas-benchmark
        namespace: "{{ ocp4_workload_litemaas_benchmark_namespace }}"
        labels:
          app: litemaas-benchmark
          guid: "{{ guid }}"
          workload: ocp4_workload_litemaas_benchmark
      spec:
        backoffLimit: 1
        ttlSecondsAfterFinished: 86400  # Keep for 24 hours
        template:
          metadata:
            labels:
              app: litemaas-benchmark
              guid: "{{ guid }}"
          spec:
            restartPolicy: Never
            initContainers:
              - name: patch-auth
                image: "{{ ocp4_workload_litemaas_benchmark_image }}"
                command:
                  - /bin/sh
                  - -c
                  - |
                    cp /opt/app-root/src/multi-turn-benchmark.py /shared/multi-turn-benchmark.py
                    sed -i '/self.seed_documents: list\[SeedDocument\] = \[\]/a\        \n        # Read API key from environment\n        self.api_key = os.environ.get("OPENAI_API_KEY", "")\n        self.headers = {}\n        if self.api_key:\n            self.headers["Authorization"] = f"Bearer {self.api_key}"' /shared/multi-turn-benchmark.py
                    sed -i 's/async with httpx.AsyncClient(verify=False, timeout=self.timeout) as client:/async with httpx.AsyncClient(verify=False, timeout=self.timeout, headers=self.headers) as client:/' /shared/multi-turn-benchmark.py
                    sed -i 's/self.client = httpx.AsyncClient(verify=False, timeout=self.timeout, limits=limits)/self.client = httpx.AsyncClient(verify=False, timeout=self.timeout, limits=limits, headers=self.headers)/' /shared/multi-turn-benchmark.py
                    cp -r /opt/app-root/src/seed-documents /shared/
                    echo "âœ“ Patched benchmark script for authentication"
                volumeMounts:
                  - name: shared-data
                    mountPath: /shared
            containers:
              - name: benchmark
                image: "{{ ocp4_workload_litemaas_benchmark_image }}"
                workingDir: /shared
                command:
                  - python3
                  - /shared/multi-turn-benchmark.py
                args:
                  - "{{ _benchmark_litellm_url }}"
                  - "--conversations"
                  - "{{ ocp4_workload_litemaas_benchmark_conversations | string }}"
                  - "--turns"
                  - "{{ _benchmark_effective_turns | string }}"
                  - "--parallel"
                  - "{{ _benchmark_parallel_workers | string }}"
                  - "--max-tokens"
                  - "{{ ocp4_workload_litemaas_benchmark_max_tokens | string }}"
                  - "--min-delay"
                  - "{{ ocp4_workload_litemaas_benchmark_min_delay | string }}"
                  - "--max-delay"
                  - "{{ ocp4_workload_litemaas_benchmark_max_delay | string }}"
                env:
                  - name: OPENAI_API_KEY
                    value: "{{ _benchmark_litellm_key }}"
                volumeMounts:
                  - name: shared-data
                    mountPath: /shared
                resources:
                  requests:
                    cpu: "500m"
                    memory: "512Mi"
                  limits:
                    cpu: "2"
                    memory: "2Gi"
            volumes:
              - name: shared-data
                emptyDir: {}

- name: Save benchmark job info
  when: ocp4_workload_litemaas_benchmark_save_results | bool
  agnosticd.core.agnosticd_user_info:
    data:
      litemaas_benchmark_namespace: "{{ ocp4_workload_litemaas_benchmark_namespace }}"
      litemaas_benchmark_job_name: "litemaas-benchmark"
      litemaas_benchmark_conversations: "{{ ocp4_workload_litemaas_benchmark_conversations }}"
      litemaas_benchmark_turns: "{{ ocp4_workload_litemaas_benchmark_turns }}"

- name: Calculate benchmark timeout
  ansible.builtin.set_fact:
    # Estimated: conversations Ã— effective_turns Ã— 10 seconds per request + 20% buffer
    _benchmark_estimated_minutes: "{{ ((ocp4_workload_litemaas_benchmark_conversations | int * _benchmark_effective_turns | int * 10 / 60) * 1.2) | int }}"
    _benchmark_retries: "{{ (((ocp4_workload_litemaas_benchmark_conversations | int * _benchmark_effective_turns | int * 10 / 60) * 1.2) * 2) | int }}"  # retries = minutes * 2 (30 second intervals)

- name: Display benchmark timeout
  ansible.builtin.debug:
    msg:
      - "Benchmark timeout calculation:"
      - "  Estimated duration: {{ _benchmark_estimated_minutes }} minutes"
      - "  Max wait time: {{ _benchmark_retries * 0.5 }} minutes"
      - "  Retries: {{ _benchmark_retries }}"

- name: Wait for benchmark to complete
  kubernetes.core.k8s_info:
    api_version: batch/v1
    kind: Job
    name: litemaas-benchmark
    namespace: "{{ ocp4_workload_litemaas_benchmark_namespace }}"
  register: benchmark_job
  until:
    - benchmark_job.resources | length > 0
    - benchmark_job.resources[0].status.succeeded is defined or benchmark_job.resources[0].status.failed is defined
  retries: "{{ _benchmark_retries | int }}"
  delay: 30
  ignore_errors: true

- name: Check if benchmark succeeded
  ansible.builtin.set_fact:
    benchmark_succeeded: "{{ (benchmark_job.resources | default([]) | length > 0) and (benchmark_job.resources[0].status.succeeded | default(0) == 1) }}"
    benchmark_failed: "{{ (benchmark_job.resources | default([]) | length > 0) and (benchmark_job.resources[0].status.failed | default(0) == 1) }}"

- name: Get benchmark pod name
  when: benchmark_succeeded or benchmark_failed
  kubernetes.core.k8s_info:
    api_version: v1
    kind: Pod
    namespace: "{{ ocp4_workload_litemaas_benchmark_namespace }}"
    label_selectors:
      - app=litemaas-benchmark
  register: benchmark_pods

- name: Fetch benchmark results
  when:
    - benchmark_pods is defined
    - benchmark_pods.resources is defined
    - benchmark_pods.resources | length > 0
    - benchmark_succeeded or benchmark_failed
  kubernetes.core.k8s_log:
    api_version: v1
    kind: Pod
    name: "{{ benchmark_pods.resources[0].metadata.name }}"
    namespace: "{{ ocp4_workload_litemaas_benchmark_namespace }}"
  register: benchmark_output
  ignore_errors: true

- name: Parse benchmark metrics
  when: benchmark_succeeded and benchmark_output is defined
  block:
    - name: Extract metrics from output
      ansible.builtin.set_fact:
        benchmark_p95: "{{ benchmark_output.log | regex_search('P95:\\s+(\\d+\\.\\d+)', '\\1') | first | default('N/A') }}"
        benchmark_p99: "{{ benchmark_output.log | regex_search('P99:\\s+(\\d+\\.\\d+)', '\\1') | first | default('N/A') }}"
        benchmark_mean: "{{ benchmark_output.log | regex_search('Mean:\\s+(\\d+\\.\\d+)', '\\1') | first | default('N/A') }}"
        benchmark_p50: "{{ benchmark_output.log | regex_search('P50:\\s+(\\d+\\.\\d+)', '\\1') | first | default('N/A') }}"
        benchmark_speedup: "{{ benchmark_output.log | regex_search('Speedup ratio:\\s+(\\d+\\.\\d+)', '\\1') | first | default('N/A') }}"
        benchmark_total_time: "{{ benchmark_output.log | regex_search('Total time:\\s+(\\d+\\.\\d+)s', '\\1') | first | default('N/A') }}"
        benchmark_total_requests: "{{ benchmark_output.log | regex_search('Total requests:\\s+(\\d+)', '\\1') | first | default('N/A') }}"
        benchmark_rps: "{{ benchmark_output.log | regex_search('Requests per second:\\s+(\\d+\\.\\d+)', '\\1') | first | default('N/A') }}"

    - name: Determine benchmark status
      ansible.builtin.set_fact:
        benchmark_status: >-
          {{ 'PASS âœ“' if (benchmark_p95 != 'N/A' and benchmark_p95 | float < ocp4_workload_litemaas_benchmark_ttft_p95_threshold_ms)
             else 'WARN âš ' if (benchmark_p95 != 'N/A' and benchmark_p95 | float < (ocp4_workload_litemaas_benchmark_ttft_p95_threshold_ms * 2))
             else 'FAIL âœ—' }}
        benchmark_cache_status: >-
          {{ 'EXCELLENT âœ“' if (benchmark_speedup != 'N/A' and benchmark_speedup | float > (ocp4_workload_litemaas_benchmark_speedup_threshold * 1.5))
             else 'GOOD âœ“' if (benchmark_speedup != 'N/A' and benchmark_speedup | float > ocp4_workload_litemaas_benchmark_speedup_threshold)
             else 'POOR âœ—' }}

    - name: Save benchmark results to user.info data
      when: ocp4_workload_litemaas_benchmark_save_results | bool
      agnosticd.core.agnosticd_user_info:
        data:
          litemaas_benchmark_p50_ms: "{{ benchmark_p50 }}"
          litemaas_benchmark_p95_ms: "{{ benchmark_p95 }}"
          litemaas_benchmark_p99_ms: "{{ benchmark_p99 }}"
          litemaas_benchmark_mean_ms: "{{ benchmark_mean }}"
          litemaas_benchmark_speedup_ratio: "{{ benchmark_speedup }}"
          litemaas_benchmark_status: "{{ benchmark_status }}"
          litemaas_benchmark_cache_status: "{{ benchmark_cache_status }}"
          litemaas_benchmark_total_time_seconds: "{{ benchmark_total_time }}"
          litemaas_benchmark_total_requests: "{{ benchmark_total_requests }}"
          litemaas_benchmark_requests_per_second: "{{ benchmark_rps }}"

- name: Display benchmark results (Success)
  when: benchmark_succeeded and benchmark_output is defined
  agnosticd.core.agnosticd_user_info:
    msg: |

      â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
      LiteMaaS Benchmark Results
      â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

      Time to First Token (TTFT):
        â€¢ Mean:           {{ benchmark_mean }} ms
        â€¢ P50 (Median):   {{ benchmark_p50 }} ms
        â€¢ P95:            {{ benchmark_p95 }} ms
        â€¢ P99:            {{ benchmark_p99 }} ms

      Cache Performance:
        â€¢ Speedup Ratio:  {{ benchmark_speedup }}x
        â€¢ Status:         {{ benchmark_cache_status }}

      Load Testing Results:
        â€¢ Total Time:     {{ benchmark_total_time }} seconds
        â€¢ Total Requests: {{ benchmark_total_requests }}
        â€¢ Requests/sec:   {{ benchmark_rps }}

      Test Configuration:
        â€¢ Conversations:  {{ ocp4_workload_litemaas_benchmark_conversations }}
        â€¢ Turns:          {{ ocp4_workload_litemaas_benchmark_turns }}
        â€¢ Parallel:       {{ ocp4_workload_litemaas_benchmark_parallel_workers }}
        â€¢ Max Tokens:     {{ ocp4_workload_litemaas_benchmark_max_tokens }}

      Overall Status: {{ benchmark_status }}

      Performance Interpretation:
        TTFT P95 < {{ ocp4_workload_litemaas_benchmark_ttft_p95_threshold_ms }}ms:   Excellent user experience
        TTFT P95 < {{ ocp4_workload_litemaas_benchmark_ttft_p95_threshold_ms * 2 }}ms:  Good user experience
        TTFT P95 > {{ ocp4_workload_litemaas_benchmark_ttft_p95_threshold_ms * 2 }}ms:  May need optimization

        Speedup > {{ ocp4_workload_litemaas_benchmark_speedup_threshold * 1.5 }}x:  Excellent prefix caching
        Speedup > {{ ocp4_workload_litemaas_benchmark_speedup_threshold }}x:    Good prefix caching
        Speedup < {{ ocp4_workload_litemaas_benchmark_speedup_threshold }}x:    Poor cache effectiveness

      Full Results:
        oc logs -n {{ ocp4_workload_litemaas_benchmark_namespace }} -l app=litemaas-benchmark

      â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

- name: Deploy benchmark report web page
  when: benchmark_succeeded and benchmark_output is defined
  block:
    - name: Set report generation timestamp
      ansible.builtin.set_fact:
        benchmark_report_timestamp: "{{ lookup('pipe', 'date -u +\"%Y-%m-%dT%H:%M:%SZ\"') }}"

    - name: Render HTML report
      ansible.builtin.template:
        src: benchmark-report.html.j2
        dest: /tmp/benchmark-report-{{ guid }}.html
      vars:
        benchmark_output_raw: "{{ benchmark_output.log }}"
        report_timestamp: "{{ benchmark_report_timestamp }}"
        litemaas_benchmark_parallel_workers: "{{ _benchmark_parallel_workers }}"
        litemaas_benchmark_effective_turns: "{{ _benchmark_effective_turns }}"

    - name: Create ConfigMap with HTML report
      kubernetes.core.k8s:
        state: present
        definition:
          apiVersion: v1
          kind: ConfigMap
          metadata:
            name: benchmark-report
            namespace: "{{ ocp4_workload_litemaas_benchmark_namespace }}"
            labels:
              app: benchmark-report
              guid: "{{ guid }}"
          data:
            index.html: "{{ lookup('file', '/tmp/benchmark-report-' + guid + '.html') }}"

    - name: Create nginx Deployment for report
      kubernetes.core.k8s:
        state: present
        definition:
          apiVersion: apps/v1
          kind: Deployment
          metadata:
            name: benchmark-report
            namespace: "{{ ocp4_workload_litemaas_benchmark_namespace }}"
            labels:
              app: benchmark-report
              guid: "{{ guid }}"
          spec:
            replicas: 1
            selector:
              matchLabels:
                app: benchmark-report
            template:
              metadata:
                labels:
                  app: benchmark-report
              spec:
                containers:
                - name: nginx
                  image: registry.access.redhat.com/ubi9/nginx-120:latest
                  ports:
                  - containerPort: 8080
                    protocol: TCP
                  volumeMounts:
                  - name: report
                    mountPath: /opt/app-root/src
                    readOnly: true
                volumes:
                - name: report
                  configMap:
                    name: benchmark-report

    - name: Create Service for report
      kubernetes.core.k8s:
        state: present
        definition:
          apiVersion: v1
          kind: Service
          metadata:
            name: benchmark-report
            namespace: "{{ ocp4_workload_litemaas_benchmark_namespace }}"
            labels:
              app: benchmark-report
              guid: "{{ guid }}"
          spec:
            selector:
              app: benchmark-report
            ports:
            - port: 8080
              targetPort: 8080
              protocol: TCP

    - name: Create Route for report
      kubernetes.core.k8s:
        state: present
        definition:
          apiVersion: route.openshift.io/v1
          kind: Route
          metadata:
            name: benchmark-report
            namespace: "{{ ocp4_workload_litemaas_benchmark_namespace }}"
            labels:
              app: benchmark-report
              guid: "{{ guid }}"
          spec:
            to:
              kind: Service
              name: benchmark-report
            port:
              targetPort: 8080
            tls:
              termination: edge
              insecureEdgeTerminationPolicy: Redirect
      register: benchmark_report_route

    - name: Get Route URL
      kubernetes.core.k8s_info:
        api_version: route.openshift.io/v1
        kind: Route
        name: benchmark-report
        namespace: "{{ ocp4_workload_litemaas_benchmark_namespace }}"
      register: report_route_info

    - name: Save report URL to user.info
      when: report_route_info.resources | length > 0
      agnosticd.core.agnosticd_user_info:
        data:
          litemaas_benchmark_report_url: "https://{{ report_route_info.resources[0].spec.host }}"

    - name: Display report URL
      when: report_route_info.resources | length > 0
      agnosticd.core.agnosticd_user_info:
        msg: |

          â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
          ğŸ“Š View Full Benchmark Report
          â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

          https://{{ report_route_info.resources[0].spec.host }}

          This interactive report includes:
            â€¢ Visual metrics and charts
            â€¢ Performance interpretation
            â€¢ Full benchmark output
            â€¢ Configuration details

          â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

- name: Display benchmark failure
  when: benchmark_failed
  agnosticd.core.agnosticd_user_info:
    msg: |

      â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
      LiteMaaS Benchmark Failed
      â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

      The benchmark job failed to complete.

      Troubleshooting:
        # Check job status
        oc get job litemaas-benchmark -n {{ ocp4_workload_litemaas_benchmark_namespace }}

        # View error logs
        oc logs -n {{ ocp4_workload_litemaas_benchmark_namespace }} -l app=litemaas-benchmark

        # Describe pod for events
        oc describe pod -n {{ ocp4_workload_litemaas_benchmark_namespace }} -l app=litemaas-benchmark

      Common Issues:
        - LiteMaaS endpoint unreachable
        - Invalid API key
        - Model not available
        - Context length exceeded (reduce max_tokens)
        - Network connectivity issues

      â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
